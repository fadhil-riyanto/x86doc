<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="style.css">
<script src="script.js"></script>
<title>VDPBF16PS - Dot Product of BF16 Pairs Accumulated Into Packed Single Precision </title></head>
<body>
<div id="head">
<a href="index.html">x86doc</a> › VDPBF16PS - Dot Product of BF16 Pairs Accumulated Into Packed Single Precision </div>
<div id="body">
<h1>VDPBF16PS—Dot Product of BF16 Pairs Accumulated Into Packed Single Precision</h1>
<table>
<tr>
<th>Opcode/Instruction</th>
<th>Op/En</th>
<th>64/32 bit Mode Support</th>
<th>CPUID Feature Flag</th>
<th>Description</th></tr>
<tr>
<td>EVEX.128.F3.0F38.W0 52 /r VDPBF16PS xmm1{k1}{z}, xmm2, xmm3/m128/m32bcst</td>
<td>A</td>
<td>V/V</td>
<td>AVX512VL AVX512_BF16</td>
<td>Multiply BF16 pairs from xmm2 and xmm3/m128, and accumulate the resulting packed single precision results in xmm1 with writemask k1.</td></tr>
<tr>
<td>EVEX.256.F3.0F38.W0 52 /r VDPBF16PS ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst</td>
<td>A</td>
<td>V/V</td>
<td>AVX512VL AVX512_BF16</td>
<td>Multiply BF16 pairs from ymm2 and ymm3/m256, and accumulate the resulting packed single precision results in ymm1 with writemask k1.</td></tr>
<tr>
<td>EVEX.512.F3.0F38.W0 52 /r VDPBF16PS zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst</td>
<td>A</td>
<td>V/V</td>
<td>AVX512F AVX512_BF16</td>
<td>Multiply BF16 pairs from zmm2 and zmm3/m512, and accumulate the resulting packed single precision results in zmm1 with writemask k1.</td></tr></table>
<h3>Instruction Operand Encoding</h3>
<table>
<tr>
<th>Op/En</th>
<th>Tuple</th>
<th>Operand 1</th>
<th>Operand 2</th>
<th>Operand 3</th>
<th>Operand 4</th></tr>
<tr>
<td>A</td>
<td>Full</td>
<td>ModRM:reg (w)</td>
<td>EVEX.vvvv (r)</td>
<td>ModRM:r/m (r)</td>
<td>N/A</td></tr></table>
<p><strong>Description</strong></p>
<p>This instruction performs a SIMD dot-product of two BF16 pairs and accumulates into a packed single precision register.</p>
<p>“Round to nearest even” rounding mode is used when doing each accumulation of the FMA. Output denormals are always flushed to zero and input denormals are always treated as zero. MXCSR is not consulted nor updated.</p>
<p>NaN propagation priorities are described in Table 5-1.</p>
<h3>Table 5-1.  NaN Propagation Priorities</h3>
<table>
<tr>
<th>NaN Priority</th>
<th>Description</th>
<th>Comments</th></tr>
<tr>
<td>1</td>
<td>src1 low is NaN</td>
<td>Lower part has priority over upper part, i.e., it overrides the upper part.</td></tr>
<tr>
<td>2</td>
<td>src2 low is NaN</td>
<td></td></tr>
<tr>
<td>3</td>
<td>src1 high is NaN</td>
<td>Upper part may be overridden if lower has NaN.</td></tr>
<tr>
<td>4</td>
<td>src2 high is NaN</td>
<td></td></tr>
<tr>
<td>5</td>
<td>srcdest is NaN</td>
<td>Dest is propagated if no NaN is encountered by src2.</td></tr></table>
<p><strong>Operation</strong></p>
<p>Define make_fp32(x):</p>
<p>// The x parameter is bfloat16. Pack it in to upper 16b of a dword. The bit pattern is a legal fp32 value. Return that bit pattern.</p>
<p>dword := 0</p>
<p>dword[31:16] := x</p>
<p>RETURN dword</p>
<p><strong>VDPBF16PS srcdest, src1, src2</strong></p>
<p>VL = (128, 256, 512)</p>
<p>KL = VL/32</p>
<p>origdest := srcdest</p>
<p>FOR i := 0 to KL-1:</p>
<p>IF k1[ i ] or *no writemask*:</p>
<p>IF src2 is memory and evex.b == 1:</p>
<p>t := src2.dword[0]</p>
<p>ELSE:</p>
<p>t := src2.dword[ i ]</p>
<p>// FP32 FMA with daz in, ftz out and RNE rounding. MXCSR neither consulted nor updated.</p>
<p>srcdest.fp32[ i ] += make_fp32(src1.bfloat16[2*i+1]) * make_fp32(t.bfloat[1])</p>
<p>srcdest.fp32[ i ] += make_fp32(src1.bfloat16[2*i+0]) * make_fp32(t.bfloat[0])</p>
<p>ELSE IF *zeroing*:</p>
<p>srcdest.dword[ i ] := 0</p>
<p>ELSE: // merge masking, dest element unchanged</p>
<p>srcdest.dword[ i ] := origdest.dword[ i ]</p>
<p>srcdest[MAXVL-1:VL] := 0</p>
<p><strong>Intel C/C++ Compiler Intrinsic Equivalent</strong></p>
<p>VDPBF16PS __m128 _mm_dpbf16_ps(__m128, __m128bh, __m128bh);</p>
<p>VDPBF16PS __m128 _mm_mask_dpbf16_ps( __m128, __mmask8, __m128bh, __m128bh);</p>
<p>VDPBF16PS __m128 _mm_maskz_dpbf16_ps(__mmask8, __m128, __m128bh, __m128bh);</p>
<p>VDPBF16PS __m256 _mm256_dpbf16_ps(__m256, __m256bh, __m256bh);</p>
<p>VDPBF16PS __m256 _mm256_mask_dpbf16_ps(__m256, __mmask8, __m256bh, __m256bh);</p>
<p>VDPBF16PS __m256 _mm256_maskz_dpbf16_ps(__mmask8, __m256, __m256bh, __m256bh);</p>
<p>VDPBF16PS __m512 _mm512_dpbf16_ps(__m512, __m512bh, __m512bh);</p>
<p>VDPBF16PS __m512 _mm512_mask_dpbf16_ps(__m512, __mmask16, __m512bh, __m512bh);</p>
<p>VDPBF16PS __m512 _mm512_maskz_dpbf16_ps(__mmask16, __m512, __m512bh, __m512bh);</p>
<p><strong>SIMD Floating-Point Exceptions</strong></p>
<p>None.</p>
<p><strong>Other Exceptions</strong></p>
<p>See Table 2-49, “Type E4 Class Exception Conditions.”</p></div></body></html>